{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "import json\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "os.chdir(\"C:/Users/Zber/Documents/Dev_program/OpenRadar\")\n",
    "from FER.utils import MapRecord, get_label\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def rename(old_name, data_folder):\n",
    "    old_path = os.path.join(data_folder, old_name)\n",
    "    new_path = old_path.replace('_2_', '_')\n",
    "    os.rename(old_path, new_path)\n",
    "    print(\"Rename from {} to {}\".format(old_path, new_path))\n",
    "\n",
    "\n",
    "def annotation_update(record_list, width=100, total_frame=300):\n",
    "    for record in record_list:\n",
    "        # if record.num_frames < width:\n",
    "        #     pad = (width - record.num_frames)//2\n",
    "        #     if record.onset < pad:\n",
    "        #         record.peak += pad*2\n",
    "\n",
    "        #     elif (total_frame - record.peak) < pad:\n",
    "        #         record.onset -= pad*2\n",
    "        #     else:\n",
    "        #         record.onset -= pad\n",
    "        #         record.peak += pad\n",
    "        # else:\n",
    "        #     pad = record.num_frames - width\n",
    "        #     record.peak -= pad\n",
    "\n",
    "        record.path = record.path.replace(\"Raw_0.bin\", \"{}.npy\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "        if record.num_frames != 100:\n",
    "            record.peak += 1\n",
    "        assert record.num_frames == 100, 'the num of frames must equal to 100!'\n",
    "    return record_list\n",
    "\n",
    "\n",
    "def annotation_attention(record_list, width=30):\n",
    "    for record in record_list:\n",
    "        record.onset = math.floor(record.onset * 3 / 10)\n",
    "        record.peak = record.onset + width - 1\n",
    "        record.path = record.relative_path.replace(\"_{}.npy\", \"\")\n",
    "    return record_list\n",
    "\n",
    "\n",
    "def annotation_append(subs=['S6', 'S7']):\n",
    "    str_arr = []\n",
    "    str_format = \"{} {} {} {} {} {} {} {}\"\n",
    "    npy_path = \"{}_{}\"\n",
    "    emotion = 'Neutral'\n",
    "    # subs = ['S0', 'S1', 'S2', 'S3', 'S4', 'S5']\n",
    "\n",
    "    for sub in subs:\n",
    "        for i in range(0, 30):\n",
    "            path = (os.path.join(sub, npy_path.format(emotion, i, )) + '_{}.npy').replace(\"\\\\\", \"/\")\n",
    "            label = \"0\"\n",
    "            onset = 31\n",
    "            peak = 130\n",
    "            offset = -1\n",
    "            e1 = 0\n",
    "            e2 = 0\n",
    "            e3 = 0\n",
    "            str_arr.append(str_format.format(path, label, onset, peak, offset, e1, e2, e3))\n",
    "    return str_arr\n",
    "\n",
    "\n",
    "def data_split(record_list):\n",
    "    labels = [r.label for r in record_list]\n",
    "    train, test = train_test_split(record_list, test_size=0.2, random_state=25, stratify=labels)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def hm_2_frame(root_path, hm_path, frame_path):\n",
    "    record_list = [MapRecord(x.strip().split(), root_path) for x in open(hm_path)]\n",
    "\n",
    "    new_record_list = annotation_attention(record_list)\n",
    "    str_format = \"{} {} {} {}\\n\"\n",
    "    with open(frame_path, 'w') as f:\n",
    "        for record in new_record_list:\n",
    "            f.write(str_format.format(record.path, record.onset, record.peak, record.label))\n",
    "    print(\"Write {} Records to txt file\".format(len(new_record_list)))\n",
    "\n",
    "\n",
    "def hm_2_landmark(root_path, hm_path, frame_path):\n",
    "    record_list = [MapRecord(x.strip().split(), root_path) for x in open(hm_path)]\n",
    "\n",
    "    new_record_list = annotation_attention(record_list)\n",
    "    str_format = \"{} {} {} {}\\n\"\n",
    "    with open(frame_path, 'w') as f:\n",
    "        for record in new_record_list:\n",
    "            re_pa = record.path.replace(\"_{}\", \"\") + \".npy\"\n",
    "            f.write(str_format.format(re_pa, record.onset, record.peak, record.label))\n",
    "    print(\"Write {} Records to txt file\".format(len(new_record_list)))\n",
    "\n",
    "\n",
    "def append_record_to_file(record_list, file):\n",
    "    str_format = \"{} {} {} {} {} {} {} {}\"\n",
    "    with open(file, 'a') as f:\n",
    "        for record in record_list:\n",
    "            f.write(str_format.format(record.path + \"_{}.npy\", record.label, record.onset, record.peak,\n",
    "                                      record.offset, record.width_err, record.height_err, record.index_err) + '\\n')\n",
    "    print(\"Write {} Records to txt file\".format(len(record_list)))\n",
    "\n",
    "\n",
    "def hm_2_landmark_v1(record_list, frame_path):\n",
    "    record_list = record_list\n",
    "\n",
    "    new_record_list = annotation_attention(record_list)\n",
    "    str_format = \"{} {} {} {}\\n\"\n",
    "    with open(frame_path, 'w') as f:\n",
    "        for record in new_record_list:\n",
    "            re_pa = record.path.replace(\"_{}\", \"\") + \".npy\"\n",
    "            f.write(str_format.format(re_pa, record.onset, record.peak, record.label))\n",
    "    print(\"Write {} Records to txt file\".format(len(new_record_list)))\n",
    "\n",
    "\n",
    "def check_is_in_badfile(bad_file_list, sub, emo, idx):\n",
    "    for bf in bad_file_list:\n",
    "        bfname = bf.replace('\\n', '').split(',')\n",
    "        if sub == bfname[0] and emo == bfname[1] and idx == int(bfname[2]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge multi text files\n",
    "root_path = \"C:\\\\Users\\\\Zber\\\\Desktop\\\\Subjects_Heatmap\"\n",
    "# st = \"heatmap_train_Standing.txt\"\n",
    "# sit = \"heatmap_train_Sitting.txt\"\n",
    "# ground = \"heatmap_train_Ground.txt\"\n",
    "# res = \"heatmap_train_Pose.txt\"\n",
    "\n",
    "\n",
    "# st = \"heatmap_test_Ground_S1.txt\"\n",
    "# sit = \"heatmap_test_Ground_S2.txt\"\n",
    "# ground = \"heatmap_test_Ground_S3.txt\"\n",
    "# res = \"heatmap_test_Ground.txt\"\n",
    "\n",
    "\n",
    "# st = \"heatmap_test_Standing.txt\"\n",
    "# sit = \"heatmap_test_Sitting.txt\"\n",
    "# ground = \"heatmap_test_Ground.txt\"\n",
    "# res = \"heatmap_test_Pose.txt\"\n",
    "\n",
    "# st = \"heatmap_test_Distance_70cm.txt\"\n",
    "# sit = \"heatmap_test_Distance_100cm.txt\"\n",
    "# ground = \"heatmap_test_Distance_150cm.txt\"\n",
    "# g1 = \"heatmap_test_Distance_200cm.txt\"\n",
    "# res = \"heatmap_test_Distance.txt\"\n",
    "\n",
    "# st = \"heatmap_train_Distance_70cm.txt\"\n",
    "# sit = \"heatmap_train_Distance_100cm.txt\"\n",
    "# ground = \"heatmap_train_Distance_150cm.txt\"\n",
    "# g1 = \"heatmap_train_Distance_200cm.txt\"\n",
    "# g2 = \"heatmap_train_Distance_250cm.txt\"\n",
    "# res = \"heatmap_train_Distance_v1.txt\"\n",
    "\n",
    "# st = \"heatmap_test_Distance_70cm.txt\"\n",
    "# sit = \"heatmap_test_Distance_100cm.txt\"\n",
    "# ground = \"heatmap_test_Distance_150cm.txt\"\n",
    "# g1 = \"heatmap_test_Distance_200cm.txt\"\n",
    "# g2 = \"heatmap_test_Distance_250cm.txt\"\n",
    "# res = \"heatmap_test_Distance_v1.txt\"\n",
    "tr = \"heatmap_test_{}.txt\"\n",
    "res = \"heatmap_test_Motion_v3.txt\"\n",
    "names = ['M1_0', 'M1_1', 'M1_2', 'M2_0', 'M2_1', 'M2_2', 'Distance_100cm']\n",
    "# names = ['M3_0', 'M3_1', 'M3_2']\n",
    "\n",
    "arr = [tr.format(n) for n in names]\n",
    "txt = []\n",
    "\n",
    "\n",
    "for a in arr:\n",
    "    with open(os.path.join(root_path, a)) as fp:\n",
    "        txt.append(fp.read())\n",
    "\n",
    "with open (os.path.join(root_path, res), 'w') as fp:\n",
    "    for t in txt:\n",
    "        fp.write(t+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 1311 Records to txt file\n",
      "Write 663 Records to txt file\n"
     ]
    }
   ],
   "source": [
    "# train_form = \"{}_train_S6_7_8_9_v1.txt\"\n",
    "# test_form = \"{}_test_S6_7_8_9_v1.txt\"\n",
    "\n",
    "train_form = \"{}_train_S10a.txt\"\n",
    "test_form = \"{}_test_S10a.txt\"\n",
    "\n",
    "root_path = \"C:/Users/Zber/Desktop/Subjects_Heatmap\"\n",
    "train_path = os.path.join(root_path, train_form.format('heatmap'))\n",
    "test_path = os.path.join(root_path, test_form.format('heatmap'))\n",
    "\n",
    "frame_root_path = \"C:/Users/Zber/Desktop/Subjects_Frames\"\n",
    "frame_train_path = os.path.join(frame_root_path, train_form.format('frames'))\n",
    "frame_test_path = os.path.join(frame_root_path, test_form.format('frames'))\n",
    "\n",
    "hm_2_frame(\"\", train_path, frame_train_path)\n",
    "hm_2_frame(\"\", test_path, frame_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check file has NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S5/Disgust_14_{}.npy\n",
      "S5/Disgust_15_{}.npy\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "# annotaton_path = \"C:\\\\Users\\\\Zber\\\\Desktop\\\\Subjects_Heatmap\\\\heatmap_annotation_full_S8.txt\"\n",
    "# annotaton_path = \"C:\\\\Users\\\\Zber\\\\Desktop\\\\Subjects_Heatmap\\\\heatmap_train_S3_4_5.txt\"\n",
    "annotaton_path = \"C:\\\\Users\\\\Zber\\\\Desktop\\\\Subjects_Heatmap\\\\heatmap_train_landmark_S5.txt\"\n",
    "root_path = \"C:\\\\Users\\\\Zber\\\\Desktop\\\\Subjects_Heatmap\"\n",
    "\n",
    "record_list = [MapRecord(x.strip().split(), root_path) for x in open(annotaton_path)]\n",
    "\n",
    "# ele = [-np.inf]\n",
    "\n",
    "for record in record_list:\n",
    "    \n",
    "    npy_path_ele = record.path.format('ele')\n",
    "    npy_path_azi = record.path.format('azi')\n",
    "\n",
    "    ele = np.load(npy_path_ele).flatten()\n",
    "    azi = np.load(npy_path_azi).flatten()\n",
    "\n",
    "    ele = np.mean(ele)\n",
    "    azi = np.mean(azi)\n",
    "\n",
    "    # if ele == np.NINF or ele==np.Inf or azi == np.NINF or azi==np.Inf:\n",
    "    if np.isnan(ele) or np.isnan(azi):\n",
    "        print(record.path.replace(root_path+\"\\\\\",\"\"))\n",
    "    if np.isinf(ele) or np.isinf(azi):\n",
    "        print(record.path.replace(root_path+\"\\\\\",\"\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0210c7e9b9cf9ed8e3f7452f6d428fef60037b48b80a0b353b2536d40dcfcdca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
